{
  "generated_at": "2026-02-28T03:22:09.999037+00:00",
  "dataset_path": "/Users/mm/Projects/academic-research-assistant/tests/eval/dataset.json",
  "output_dir": "/Users/mm/Projects/academic-research-assistant/results",
  "total_questions": 50,
  "metrics": {
    "citation_precision": 0.0,
    "claim_support_ratio": 0.0,
    "tool_success_rate": 0.0,
    "latency_p95": 18.151169
  },
  "by_tier": {
    "tier_1": {
      "questions": 20,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 18.151169
    },
    "tier_2": {
      "questions": 20,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 17.945246
    },
    "tier_3": {
      "questions": 10,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 18.424729
    }
  },
  "results": [
    {
      "query": "Who introduced the Transformer architecture in 'Attention Is All You Need' (2017)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 16.212748,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What objective is optimized during BERT pretraining?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 9.725339,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "In which year was 'Deep Residual Learning for Image Recognition' published?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 8.596093,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who are the primary authors of the original GAN paper (2014)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 7.968433,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What benchmark did AlexNet famously win in 2012?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.661148,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What problem does the paper 'Adam: A Method for Stochastic Optimization' address?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.62312,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who proposed the U-Net architecture and for what domain?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.648656,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What does the acronym YOLO stand for in the 2016 paper?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.699098,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Which paper introduced Batch Normalization and what was its core benefit?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.556428,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the main contribution of the Word2Vec papers by Mikolov et al.?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 13.301211,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "In what year did the original LSTM paper appear?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 13.837268,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who introduced Dropout and why was it effective?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.616731,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What dataset was introduced with the MNIST paper?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.475508,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the central idea in 'Distilling the Knowledge in a Neural Network'?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 5.787169,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who proposed Layer Normalization and for what setting was it motivated?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 17.519021,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What was the key idea of the original seq2seq paper by Sutskever et al.?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 10.431204,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What does 'Neural Machine Translation by Jointly Learning to Align and Translate' introduce?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 18.151169,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who authored 'Playing Atari with Deep Reinforcement Learning'?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 16.940471,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What contribution did 'A Survey of Active Learning' (Settles, 2009) make?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 18.366913,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the purpose of the BLEU metric introduced by Papineni et al. (2002)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 12.549399,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare BERT and RoBERTa pretraining strategies and explain why RoBERTa improved performance.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 12.94086,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Transformer self-attention differ from RNN-based sequence modeling in computational tradeoffs?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 16.114757,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain the difference between variational autoencoders and GANs in training objectives and output quality.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 10.529613,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare ResNet skip connections with DenseNet connectivity; when is each advantageous?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 13.190875,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Batch Normalization differ from Layer Normalization for training stability?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 5.704682,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain how Adam differs from SGD with momentum and when Adam can generalize worse.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 12.689557,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare GPT-style causal language modeling with BERT masked language modeling.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 6.611273,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How do graph neural networks (GCN) differ from graph attention networks (GAT)?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 9.900003,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain retrieval-augmented generation (RAG) versus pure parametric LLMs for factual QA.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 17.863717,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare PPO and DQN for reinforcement learning: assumptions, stability, and action spaces.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 10.103528,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does contrastive learning in SimCLR differ from supervised classification training?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 14.845977,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain the differences between BLEU, ROUGE, and BERTScore for text evaluation.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 9.982544,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare CRF-based sequence labeling with Transformer token classification.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 8.651103,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How do sparse attention variants like Longformer differ from full attention Transformers?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 5.635538,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain why LoRA fine-tuning is more parameter-efficient than full-model fine-tuning.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 17.814838,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare mixture-of-experts Transformers with dense Transformers.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 15.726617,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Monte Carlo dropout approximate Bayesian uncertainty estimation?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 18.113389,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare BM25 and dense vector retrieval for scientific search.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 15.618314,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain differences between chain-of-thought prompting and tool-augmented reasoning.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 17.945246,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare k-fold cross-validation with a single train/validation/test split in ML experiments.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 12.87408,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Synthesize findings from Transformer, BERT, and T5 papers: what scaling patterns and limitations emerge?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 9.202916,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Across GAN, diffusion, and autoregressive generation papers, what open problems remain in controllability?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 13.585595,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What research gaps appear when comparing robustness papers on adversarial training and out-of-distribution detection?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 17.557398,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Integrate evidence from retrieval-augmented QA papers: where do current citation-grounding methods fail?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 5.629372,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "From RLHF and constitutional AI literature, what unresolved alignment tradeoffs are most significant?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 18.022986,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare multimodal papers (CLIP, Flamingo, GPT-4V-like systems): what bottlenecks persist for reasoning?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 16.287644,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Synthesize methods for efficient adaptation (adapters, LoRA, prefix tuning): what theory gaps remain?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 18.424729,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Across graph representation learning papers, what are the main limits of message-passing architectures?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 17.467875,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What cross-paper evidence suggests weaknesses in current LLM evaluation benchmarks?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 17.240384,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Based on reproducibility studies in ML, where should new tooling or standards focus next?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 5.485101,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    }
  ],
  "artifacts": {
    "json": "/Users/mm/Projects/academic-research-assistant/results/eval_results.json",
    "markdown": "/Users/mm/Projects/academic-research-assistant/results/eval_summary.md"
  }
}