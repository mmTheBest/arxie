{
  "generated_at": "2026-02-28T04:10:18.491048+00:00",
  "dataset_path": "/Users/mm/Projects/academic-research-assistant/tests/eval/dataset.json",
  "output_dir": "/Users/mm/Projects/academic-research-assistant/results",
  "total_questions": 50,
  "metrics": {
    "citation_precision": 0.0,
    "claim_support_ratio": 0.0,
    "tool_success_rate": 0.0,
    "latency_p95": 65.058724
  },
  "by_tier": {
    "tier_1": {
      "questions": 20,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 65.058724
    },
    "tier_2": {
      "questions": 20,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 58.303537
    },
    "tier_3": {
      "questions": 10,
      "citation_precision": 0.0,
      "claim_support_ratio": 0.0,
      "tool_success_rate": 0.0,
      "latency_p95": 59.420572
    }
  },
  "results": [
    {
      "query": "Who introduced the Transformer architecture in 'Attention Is All You Need' (2017)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 41.807826,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What objective is optimized during BERT pretraining?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 40.489842,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "In which year was 'Deep Residual Learning for Image Recognition' published?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 63.879578,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who are the primary authors of the original GAN paper (2014)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 52.548296,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What benchmark did AlexNet famously win in 2012?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 58.882968,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What problem does the paper 'Adam: A Method for Stochastic Optimization' address?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 56.829401,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who proposed the U-Net architecture and for what domain?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 48.06741,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What does the acronym YOLO stand for in the 2016 paper?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 39.268185,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Which paper introduced Batch Normalization and what was its core benefit?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 48.102673,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the main contribution of the Word2Vec papers by Mikolov et al.?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 65.058724,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "In what year did the original LSTM paper appear?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 57.508514,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who introduced Dropout and why was it effective?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 52.3673,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What dataset was introduced with the MNIST paper?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 69.723768,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the central idea in 'Distilling the Knowledge in a Neural Network'?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 52.900638,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who proposed Layer Normalization and for what setting was it motivated?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 38.91953,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What was the key idea of the original seq2seq paper by Sutskever et al.?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 60.013254,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What does 'Neural Machine Translation by Jointly Learning to Align and Translate' introduce?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 57.301043,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Who authored 'Playing Atari with Deep Reinforcement Learning'?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 53.421891,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What contribution did 'A Survey of Active Learning' (Settles, 2009) make?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 44.704718,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What is the purpose of the BLEU metric introduced by Papineni et al. (2002)?",
      "difficulty_tier": "tier_1",
      "latency_seconds": 62.013312,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare BERT and RoBERTa pretraining strategies and explain why RoBERTa improved performance.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 67.24221,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Transformer self-attention differ from RNN-based sequence modeling in computational tradeoffs?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 58.303537,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain the difference between variational autoencoders and GANs in training objectives and output quality.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 41.739104,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare ResNet skip connections with DenseNet connectivity; when is each advantageous?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 57.569146,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Batch Normalization differ from Layer Normalization for training stability?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 44.542363,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain how Adam differs from SGD with momentum and when Adam can generalize worse.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 45.261594,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare GPT-style causal language modeling with BERT masked language modeling.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 49.821763,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How do graph neural networks (GCN) differ from graph attention networks (GAT)?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 38.342158,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain retrieval-augmented generation (RAG) versus pure parametric LLMs for factual QA.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 37.286723,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare PPO and DQN for reinforcement learning: assumptions, stability, and action spaces.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 43.625989,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does contrastive learning in SimCLR differ from supervised classification training?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 29.539183,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain the differences between BLEU, ROUGE, and BERTScore for text evaluation.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 40.042055,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare CRF-based sequence labeling with Transformer token classification.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 45.800484,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How do sparse attention variants like Longformer differ from full attention Transformers?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 35.098476,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain why LoRA fine-tuning is more parameter-efficient than full-model fine-tuning.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 39.547756,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare mixture-of-experts Transformers with dense Transformers.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 48.335279,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "How does Monte Carlo dropout approximate Bayesian uncertainty estimation?",
      "difficulty_tier": "tier_2",
      "latency_seconds": 33.370251,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare BM25 and dense vector retrieval for scientific search.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 34.247053,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Explain differences between chain-of-thought prompting and tool-augmented reasoning.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 28.877345,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare k-fold cross-validation with a single train/validation/test split in ML experiments.",
      "difficulty_tier": "tier_2",
      "latency_seconds": 47.626095,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Synthesize findings from Transformer, BERT, and T5 papers: what scaling patterns and limitations emerge?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 29.134979,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Across GAN, diffusion, and autoregressive generation papers, what open problems remain in controllability?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 33.398853,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What research gaps appear when comparing robustness papers on adversarial training and out-of-distribution detection?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 38.166933,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Integrate evidence from retrieval-augmented QA papers: where do current citation-grounding methods fail?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 32.464414,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "From RLHF and constitutional AI literature, what unresolved alignment tradeoffs are most significant?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 59.420572,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Compare multimodal papers (CLIP, Flamingo, GPT-4V-like systems): what bottlenecks persist for reasoning?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 38.678269,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Synthesize methods for efficient adaptation (adapters, LoRA, prefix tuning): what theory gaps remain?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 29.374393,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Across graph representation learning papers, what are the main limits of message-passing architectures?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 40.76627,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "What cross-paper evidence suggests weaknesses in current LLM evaluation benchmarks?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 35.243779,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    },
    {
      "query": "Based on reproducibility studies in ML, where should new tooling or standards focus next?",
      "difficulty_tier": "tier_3",
      "latency_seconds": 36.039893,
      "inline_citations": 0,
      "matched_citations": 0,
      "citation_precision": 0.0,
      "claim_count": 2,
      "supported_claim_count": 0,
      "claim_support_ratio": 0.0,
      "keyword_coverage": 0.0,
      "citation_count_ok": false,
      "tool_calls": 0,
      "tool_successes": 0,
      "error": null
    }
  ],
  "artifacts": {
    "json": "/Users/mm/Projects/academic-research-assistant/results/eval_results.json",
    "markdown": "/Users/mm/Projects/academic-research-assistant/results/eval_summary.md"
  }
}