> NOTE (2026-02-28): These baseline metrics were generated before scoring fixes in `tests/eval/harness.py` and must be re-run to produce updated values.

# Evaluation Summary

- Generated at (UTC): `2026-02-28T04:10:18.491048+00:00`
- Dataset: `/Users/mm/Projects/academic-research-assistant/tests/eval/dataset.json`
- Total questions: `50`

## Topline Metrics

| Metric | Value |
|---|---:|
| citation_precision | 0.000000 |
| claim_support_ratio | 0.000000 |
| tool_success_rate | 0.000000 |
| latency_p95 (seconds) | 65.058724 |

## Tier Breakdown

| Tier | Questions | Citation Precision | Claim Support | Tool Success | Latency p95 |
|---|---:|---:|---:|---:|---:|
| tier_1 | 20 | 0.000000 | 0.000000 | 0.000000 | 65.058724 |
| tier_2 | 20 | 0.000000 | 0.000000 | 0.000000 | 58.303537 |
| tier_3 | 10 | 0.000000 | 0.000000 | 0.000000 | 59.420572 |
